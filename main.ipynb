{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_loader import get_data_loader\n",
    "\n",
    "from models.dcgan import DCGAN_MODEL\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "import time as t\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from utils.tensorboard_logger import Logger\n",
    "from itertools import chain\n",
    "from torchvision import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SAVE_PER_TIMES = 100\n",
    "\n",
    "class WGAN_Generator(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        # Filters [1024, 512, 256]\n",
    "        # Input_dim = 100\n",
    "        # Output_dim = C (number of channels)\n",
    "        self.main_module = nn.Sequential(\n",
    "            # Z latent vector 100\n",
    "            nn.ConvTranspose2d(in_channels=100, out_channels=1024, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features=1024),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # State (1024x4x4)\n",
    "            nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # State (512x8x8)\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # State (256x16x16)\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=channels, kernel_size=4, stride=2, padding=1))\n",
    "            # output of main module --> Image (Cx32x32)\n",
    "\n",
    "        self.output = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main_module(x)\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "class WGAN_Discriminator(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        # Filters [256, 512, 1024]\n",
    "        # Input_dim = channels (Cx64x64)\n",
    "        # Output_dim = 1\n",
    "        self.main_module = nn.Sequential(\n",
    "            # Omitting batch normalization in critic because our new penalized training objective (WGAN with gradient penalty) is no longer valid\n",
    "            # in this setting, since we penalize the norm of the critic's gradient with respect to each input independently and not the enitre batch.\n",
    "            # There is not good & fast implementation of layer normalization --> using per instance normalization nn.InstanceNorm2d()\n",
    "            # Image (Cx32x32)\n",
    "            nn.Conv2d(in_channels=channels, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # State (256x16x16)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(512, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # State (512x8x8)\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(1024, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "            # output of main module --> State (1024x4x4)\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            # The output of D is no longer a probability, we do not apply sigmoid at the output of D.\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1, kernel_size=4, stride=1, padding=0))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main_module(x)\n",
    "        return self.output(x)\n",
    "\n",
    "    def feature_extraction(self, x):\n",
    "        # Use discriminator for feature extraction then flatten to vector of 16384\n",
    "        x = self.main_module(x)\n",
    "        return x.view(-1, 1024*4*4)\n",
    "\n",
    "\n",
    "class WGAN_GP(object):\n",
    "    def __init__(self, args):\n",
    "        print(\"WGAN_GradientPenalty init model.\")\n",
    "        self.G = WGAN_Generator(args.channels)\n",
    "        self.D = WGAN_Discriminator(args.channels)\n",
    "        self.C = args.channels\n",
    "\n",
    "        # Check if cuda is available\n",
    "        self.check_cuda(args.cuda)\n",
    "\n",
    "        # WGAN values from paper\n",
    "        self.learning_rate = 1e-4\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.batch_size = 64\n",
    "\n",
    "        # WGAN_gradient penalty uses ADAM\n",
    "        self.d_optimizer = optim.Adam(self.D.parameters(), lr=self.learning_rate, betas=(self.b1, self.b2))\n",
    "        self.g_optimizer = optim.Adam(self.G.parameters(), lr=self.learning_rate, betas=(self.b1, self.b2))\n",
    "\n",
    "        # Set the logger\n",
    "        self.logger = Logger('./logs')\n",
    "        self.logger.writer.flush()\n",
    "        self.number_of_images = 10\n",
    "\n",
    "        self.generator_iters = args.generator_iters\n",
    "        self.critic_iter = 5\n",
    "        self.lambda_term = 10\n",
    "\n",
    "    def get_torch_variable(self, arg):\n",
    "        if self.cuda:\n",
    "            return Variable(arg).cuda(self.cuda_index)\n",
    "        else:\n",
    "            return Variable(arg)\n",
    "\n",
    "    def check_cuda(self, cuda_flag=False):\n",
    "        print(cuda_flag)\n",
    "        if cuda_flag:\n",
    "            self.cuda_index = 0\n",
    "            self.cuda = True\n",
    "            self.D.cuda(self.cuda_index)\n",
    "            self.G.cuda(self.cuda_index)\n",
    "            print(\"Cuda enabled flag: {}\".format(self.cuda))\n",
    "        else:\n",
    "            self.cuda = False\n",
    "\n",
    "\n",
    "    def train(self, train_loader):\n",
    "        print(\"STARTING\")\n",
    "        # Now batches are callable self.data.next()\n",
    "        self.data = self.get_infinite_batches(train_loader)\n",
    "        results = [[],[]]\n",
    "        one = torch.tensor(1, dtype=torch.float)\n",
    "        mone = one * -1\n",
    "        if self.cuda:\n",
    "            one = one.cuda(self.cuda_index)\n",
    "            mone = mone.cuda(self.cuda_index)\n",
    "\n",
    "        for g_iter in range(self.generator_iters):\n",
    "            # Requires grad, Generator requires_grad = False\n",
    "            for p in self.D.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "            d_loss_real = 0\n",
    "            d_loss_fake = 0\n",
    "            Wasserstein_D = 0\n",
    "            # Train Dicriminator forward-loss-backward-update self.critic_iter times while 1 Generator forward-loss-backward-update\n",
    "            for d_iter in range(self.critic_iter):\n",
    "                print(d_iter)\n",
    "                self.D.zero_grad()\n",
    "\n",
    "                images = self.data.__next__()\n",
    "                # Check for batch to have full batch_size\n",
    "                if (images.size()[0] != self.batch_size):\n",
    "                    continue\n",
    "\n",
    "                z = torch.rand((self.batch_size, 100, 1, 1))\n",
    "\n",
    "                images, z = self.get_torch_variable(images), self.get_torch_variable(z)\n",
    "\n",
    "                # Train discriminator\n",
    "                # WGAN - Training discriminator more iterations than generator\n",
    "                # Train with real images\n",
    "                d_loss_real = self.D(images)\n",
    "                d_loss_real = d_loss_real.mean()\n",
    "                d_loss_real.backward(mone)\n",
    "\n",
    "                # Train with fake images\n",
    "                z = self.get_torch_variable(torch.randn(self.batch_size, 100, 1, 1))\n",
    "\n",
    "                fake_images = self.G(z)\n",
    "                d_loss_fake = self.D(fake_images)\n",
    "                d_loss_fake = d_loss_fake.mean()\n",
    "                d_loss_fake.backward(one)\n",
    "\n",
    "                # Train with gradient penalty\n",
    "                gradient_penalty = self.calculate_gradient_penalty(images.data, fake_images.data)\n",
    "                gradient_penalty.backward()\n",
    "\n",
    "\n",
    "                d_loss = d_loss_fake - d_loss_real + gradient_penalty\n",
    "                Wasserstein_D = d_loss_real - d_loss_fake\n",
    "                self.d_optimizer.step()\n",
    "                print(f'  Discriminator iteration: {d_iter}/{self.critic_iter}, loss_fake: {d_loss_fake}, loss_real: {d_loss_real}')\n",
    "\n",
    "            # Generator update\n",
    "            for p in self.D.parameters():\n",
    "                p.requires_grad = False  # to avoid computation\n",
    "\n",
    "            self.G.zero_grad()\n",
    "            # train generator\n",
    "            # compute loss with fake images\n",
    "            z = self.get_torch_variable(torch.randn(self.batch_size, 100, 1, 1))\n",
    "            fake_images = self.G(z)\n",
    "            g_loss = self.D(fake_images)\n",
    "            g_loss = g_loss.mean()\n",
    "            g_loss.backward(mone)\n",
    "            g_cost = -g_loss\n",
    "            self.g_optimizer.step()\n",
    "            print(f'Generator iteration: {g_iter}/{self.generator_iters}, g_loss: {g_loss}')\n",
    "            # Saving model and sampling images every 1000th generator iterations\n",
    "            if (g_iter) % SAVE_PER_TIMES == 0:\n",
    "                results[0].append((float(g_loss))\n",
    "                results[1].append(float(d_loss)))\n",
    "                print(results[-1])\n",
    "\n",
    "                # Denormalize images and save them in grid 8x8\n",
    "                # Testing\n",
    "                #print(\"Real Inception score: {}\".format(inception_score))\n",
    "                print(\"Generator iter: {}\".format(g_iter))\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    def calculate_gradient_penalty(self, real_images, fake_images):\n",
    "        eta = torch.FloatTensor(self.batch_size,1,1,1).uniform_(0,1)\n",
    "        eta = eta.expand(self.batch_size, real_images.size(1), real_images.size(2), real_images.size(3))\n",
    "        if self.cuda:\n",
    "            eta = eta.cuda(self.cuda_index)\n",
    "        else:\n",
    "            eta = eta\n",
    "\n",
    "        interpolated = eta * real_images + ((1 - eta) * fake_images)\n",
    "\n",
    "        if self.cuda:\n",
    "            interpolated = interpolated.cuda(self.cuda_index)\n",
    "        else:\n",
    "            interpolated = interpolated\n",
    "\n",
    "        # define it to calculate gradient\n",
    "        interpolated = Variable(interpolated, requires_grad=True)\n",
    "\n",
    "        # calculate probability of interpolated examples\n",
    "        prob_interpolated = self.D(interpolated)\n",
    "\n",
    "        # calculate gradients of probabilities with respect to examples\n",
    "        gradients = autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                               grad_outputs=torch.ones(\n",
    "                                   prob_interpolated.size()).cuda(self.cuda_index) if self.cuda else torch.ones(\n",
    "                                   prob_interpolated.size()),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "        grad_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * self.lambda_term\n",
    "        return grad_penalty\n",
    "\n",
    "    def real_images(self, images, number_of_images):\n",
    "        if (self.C == 3):\n",
    "            return self.to_np(images.view(-1, self.C, 32, 32)[:self.number_of_images])\n",
    "        else:\n",
    "            return self.to_np(images.view(-1, 32, 32)[:self.number_of_images])\n",
    "\n",
    "    def generate_img(self, z, number_of_images):\n",
    "        samples = self.G(z).data.cpu().numpy()[:number_of_images]\n",
    "        generated_images = []\n",
    "        for sample in samples:\n",
    "            if self.C == 3:\n",
    "                generated_images.append(sample.reshape(self.C, 32, 32))\n",
    "            else:\n",
    "                generated_images.append(sample.reshape(32, 32))\n",
    "        return generated_images\n",
    "\n",
    "    def to_np(self, x):\n",
    "        return x.data.cpu().numpy()\n",
    "\n",
    "\n",
    "    def get_infinite_batches(self, data_loader):\n",
    "        while True:\n",
    "            for i, (images, _) in enumerate(data_loader):\n",
    "                yield images\n",
    "\n",
    "    def generate_latent_walk(self, number):\n",
    "        if not os.path.exists('interpolated_images/'):\n",
    "            os.makedirs('interpolated_images/')\n",
    "\n",
    "        number_int = 10\n",
    "        # interpolate between twe noise(z1, z2).\n",
    "        z_intp = torch.FloatTensor(1, 100, 1, 1)\n",
    "        z1 = torch.randn(1, 100, 1, 1)\n",
    "        z2 = torch.randn(1, 100, 1, 1)\n",
    "        if self.cuda:\n",
    "            z_intp = z_intp.cuda()\n",
    "            z1 = z1.cuda()\n",
    "            z2 = z2.cuda()\n",
    "\n",
    "        z_intp = Variable(z_intp)\n",
    "        images = []\n",
    "        alpha = 1.0 / float(number_int + 1)\n",
    "        print(alpha)\n",
    "        for i in range(1, number_int + 1):\n",
    "            z_intp.data = z1*alpha + z2*(1.0 - alpha)\n",
    "            alpha += alpha\n",
    "            fake_im = self.G(z_intp)\n",
    "            fake_im = fake_im.mul(0.5).add(0.5) #denormalize\n",
    "            images.append(fake_im.view(self.C,32,32).data.cpu())\n",
    "\n",
    "        grid = utils.make_grid(images, nrow=number_int )\n",
    "        utils.save_image(grid, 'interpolated_images/interpolated_{}.png'.format(str(number).zfill(3)))\n",
    "        print(\"Saved interpolated images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    model = None\n",
    "    if args.model == 'DCGAN':\n",
    "        model = DCGAN_MODEL(args)\n",
    "    elif args.model == 'WGAN':\n",
    "        model = WGAN_GP(args)\n",
    "    else:\n",
    "        print(\"Model type non-existing. Try again.\")\n",
    "        return 0\n",
    "\n",
    "    print(model)\n",
    "    # Load datasets to train and test loaders\n",
    "    train_loader, test_loader = get_data_loader(args)\n",
    "    #feature_extraction = FeatureExtractionTest(train_loader, test_loader, args.cuda, args.batch_size)\n",
    "\n",
    "    # Start model training\n",
    "    results = model.train(train_loader)\n",
    "    for i in range(5):\n",
    "        model.generate_latent_walk(i)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "if DEBUG:\n",
    "    ITERS = \"2\"\n",
    "    EPOCHS = \"2\"\n",
    "    CUDA = False\n",
    "else:\n",
    "    ITERS = \"40000\"\n",
    "    EPOCHS = \"400\"\n",
    "    CUDA = True\n",
    "\n",
    "def parse_args(args=None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model', type=str, default='DCGAN', choices=['DCGAN', 'WGAN'])\n",
    "    parser.add_argument('--is_train', type=str, default='True')\n",
    "    parser.add_argument('--dataroot', required=True)\n",
    "    parser.add_argument('--dataset', type=str, default='mnist', choices=['fashion-mnist'])\n",
    "    parser.add_argument('--download', type=str, default='False')\n",
    "    parser.add_argument('--epochs', type=int, default=50)\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--cuda',  type=bool, default='False')\n",
    "    parser.add_argument('--load_D', type=str, default='False')\n",
    "    parser.add_argument('--load_G', type=str, default='False')\n",
    "    parser.add_argument('--generator_iters', type=int, default=10000)\n",
    "    parser.add_argument('--channels',type=int,default=1)\n",
    "    return parser.parse_args(args)\n",
    "\n",
    " \n",
    "    \n",
    "wgan_args = [\"--model\", \"WGAN\",\n",
    "           \"--is_train\", \"True\",\n",
    "           \"--epochs\", EPOCHS,\n",
    "           \"--cuda\",CUDA,\n",
    "\n",
    "           \"--download\", \"False\",\n",
    "           \"--dataroot\", \"datasets/fashion-mnist\",\n",
    "           \"--dataset\", \"fashion-mnist\",\n",
    "           \"--generator_iters\", ITERS,\n",
    "           \"--cuda\", \"False\",\n",
    "           \"--batch_size\" ,\"64\"]\n",
    "\n",
    "dcgan_args = [\"--model\", \"DCGAN\",\n",
    "           \"--is_train\", \"True\",\n",
    "           \"--epochs\", EPOCHS,\n",
    "           \"--cuda\",CUDA,\n",
    "           \"--download\", \"False\",\n",
    "           \"--dataroot\", \"datasets/fashion-mnist\",\n",
    "           \"--dataset\", \"fashion-mnist\",\n",
    "           \"--generator_iters\", ITERS,\n",
    "           \"--cuda\", \"False\",\n",
    "           \"--batch_size\" ,\"64\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WGAN_GradientPenalty init model.\n",
      "False\n",
      "<__main__.WGAN_GP object at 0x0000012D64059908>\n",
      "STARTING\n",
      "0\n",
      "  Discriminator iteration: 0/5, loss_fake: -0.06678056716918945, loss_real: -0.273993581533432\n",
      "1\n",
      "  Discriminator iteration: 1/5, loss_fake: -2.2785754203796387, loss_real: 5.808892250061035\n",
      "2\n",
      "  Discriminator iteration: 2/5, loss_fake: -4.645873069763184, loss_real: 10.705155372619629\n",
      "3\n",
      "  Discriminator iteration: 3/5, loss_fake: -7.492238998413086, loss_real: 14.515768051147461\n",
      "4\n",
      "  Discriminator iteration: 4/5, loss_fake: -10.970818519592285, loss_real: 17.995986938476562\n",
      "Generator iteration: 0/2, g_loss: -14.342090606689453\n",
      "(tensor(-14.3421, grad_fn=<MeanBackward0>), tensor(-20.2938, grad_fn=<AddBackward0>))\n",
      "Generator iter: 0\n",
      "0\n",
      "  Discriminator iteration: 0/5, loss_fake: 12.766979217529297, loss_real: 20.55562973022461\n",
      "1\n",
      "  Discriminator iteration: 1/5, loss_fake: 6.900254249572754, loss_real: 23.583248138427734\n",
      "2\n",
      "  Discriminator iteration: 2/5, loss_fake: -3.0525715351104736, loss_real: 23.275251388549805\n",
      "3\n",
      "  Discriminator iteration: 3/5, loss_fake: -12.611238479614258, loss_real: 23.42556381225586\n",
      "4\n",
      "  Discriminator iteration: 4/5, loss_fake: -19.18022346496582, loss_real: 25.621915817260742\n",
      "Generator iteration: 1/2, g_loss: -23.257322311401367\n",
      "0.09090909090909091\n",
      "Saved interpolated images.\n",
      "0.09090909090909091\n",
      "Saved interpolated images.\n",
      "0.09090909090909091\n",
      "Saved interpolated images.\n",
      "0.09090909090909091\n",
      "Saved interpolated images.\n",
      "0.09090909090909091\n",
      "Saved interpolated images.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(tensor(-14.3421, grad_fn=<MeanBackward0>),\n",
       "  tensor(-20.2938, grad_fn=<AddBackward0>))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = parse_args(wgan_args)\n",
    "args.cuda = CUDA\n",
    "results = main(args)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCGAN model initalization.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-5bc19c357825>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdcgan_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-9f45f5809f25>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DCGAN'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDCGAN_MODEL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'WGAN'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWGAN_GP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Local\\wgan\\models\\dcgan.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;31m# check if cuda is available\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_cuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;31m# Using lower learning rate than suggested by (ADAM authors) lr=0.0002  and Beta_1 = 0.5 instead od 0.9 works better [Radford2015]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Local\\wgan\\models\\dcgan.py\u001b[0m in \u001b[0;36mcheck_cuda\u001b[1;34m(self, cuda_flag)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcuda_flag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \"\"\"\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \"\"\"\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    147\u001b[0m             raise RuntimeError(\n\u001b[0;32m    148\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             raise AssertionError(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "args = parse_args(dcgan_args)\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-14.342090606689453 -20.2938175201416\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-5b71827a6723>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "final_\n",
    "for a,b in results:\n",
    "    a,b = print(float(a),float(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitza\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.backends.backend_a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-5bc4dd70cd4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mswitch_backend\u001b[1;34m(newbackend)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[0mbackend_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend_module_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m     \u001b[1;32mclass\u001b[0m \u001b[0mbackend_mod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend_bases\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m         \u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mbackend_mod\u001b[1;34m()\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mbackend_mod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend_bases\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[0mrequired_framework\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_required_interactive_framework\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend_mod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib.backends.backend_a'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
